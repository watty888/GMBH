---
type: permanent
tags: [prompt-qc, mental-model]
---
English has the densest, cleanest training signal — especially for technical concepts, cutting-edge frameworks, niche jargon, and fresh internet culture.

Consequences:
- English prompts map more directly to well-learned patterns.
- Non-English prompts sometimes route through slightly fuzzier representations.
- Ambiguity increases if a concept exists cleanly in English but only approximately in another language.

If a model seems "dumber" in another language, it's not because the language is inferior — it's because **less high-quality text exists about that topic in that language**.
