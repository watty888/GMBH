English has the **densest**, **cleanest** **training** **signal**, especially for:
* technical concepts,
* cutting-edge frameworks,
* niche jargon,
* fresh internet culture.

That means:
- English prompts often map more directly to well-learned patterns.
- Non-English prompts sometimes route through slightly fuzzier representations.
- **Ambiguity increases if a concept exists cleanly in English but only approximately in another language.**

If a model ever seems “dumber” in another language, it’s not because the language is inferior — it’s because **less high-quality text exists about that topic in that language**.